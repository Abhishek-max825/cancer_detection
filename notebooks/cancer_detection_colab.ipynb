{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Cancer Detection - High Accuracy Training (DenseNet121)\n",
                "\n",
                "This notebook trains a DenseNet121 model on the Histopathologic Cancer Detection dataset. \n",
                "Run this in Google Colab with a GPU runtime for best results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: pandas in c:\\users\\abhis\\onedrive\\\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\\cancer detection\\.venv\\lib\\site-packages (2.3.3)\n",
                        "Requirement already satisfied: numpy in c:\\users\\abhis\\onedrive\\\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\\cancer detection\\.venv\\lib\\site-packages (2.2.6)\n",
                        "Requirement already satisfied: opencv-python in c:\\users\\abhis\\onedrive\\\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\\cancer detection\\.venv\\lib\\site-packages (4.12.0.88)\n",
                        "Requirement already satisfied: matplotlib in c:\\users\\abhis\\onedrive\\\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\\cancer detection\\.venv\\lib\\site-packages (3.10.8)\n",
                        "Requirement already satisfied: tqdm in c:\\users\\abhis\\onedrive\\\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\\cancer detection\\.venv\\lib\\site-packages (4.67.1)\n",
                        "Requirement already satisfied: scikit-learn in c:\\users\\abhis\\onedrive\\\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\\cancer detection\\.venv\\lib\\site-packages (1.8.0)\n",
                        "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\abhis\\onedrive\\\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\\cancer detection\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
                        "Requirement already satisfied: pytz>=2020.1 in c:\\users\\abhis\\onedrive\\\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\\cancer detection\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
                        "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\abhis\\onedrive\\\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\\cancer detection\\.venv\\lib\\site-packages (from pandas) (2025.3)\n",
                        "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\abhis\\onedrive\\\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\\cancer detection\\.venv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
                        "Requirement already satisfied: cycler>=0.10 in c:\\users\\abhis\\onedrive\\\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\\cancer detection\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
                        "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\abhis\\onedrive\\\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\\cancer detection\\.venv\\lib\\site-packages (from matplotlib) (4.61.1)\n",
                        "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\abhis\\onedrive\\\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\\cancer detection\\.venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
                        "Requirement already satisfied: packaging>=20.0 in c:\\users\\abhis\\onedrive\\\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\\cancer detection\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
                        "Requirement already satisfied: pillow>=8 in c:\\users\\abhis\\onedrive\\\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\\cancer detection\\.venv\\lib\\site-packages (from matplotlib) (12.1.0)\n",
                        "Requirement already satisfied: pyparsing>=3 in c:\\users\\abhis\\onedrive\\\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\\cancer detection\\.venv\\lib\\site-packages (from matplotlib) (3.3.1)\n",
                        "Requirement already satisfied: colorama in c:\\users\\abhis\\onedrive\\\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\\cancer detection\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
                        "Requirement already satisfied: scipy>=1.10.0 in c:\\users\\abhis\\onedrive\\\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\\cancer detection\\.venv\\lib\\site-packages (from scikit-learn) (1.17.0)\n",
                        "Requirement already satisfied: joblib>=1.3.0 in c:\\users\\abhis\\onedrive\\\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\\cancer detection\\.venv\\lib\\site-packages (from scikit-learn) (1.5.3)\n",
                        "Requirement already satisfied: threadpoolctl>=3.2.0 in c:\\users\\abhis\\onedrive\\\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\\cancer detection\\.venv\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
                        "Requirement already satisfied: six>=1.5 in c:\\users\\abhis\\onedrive\\\u30c9\u30ad\u30e5\u30e1\u30f3\u30c8\\cancer detection\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
                    ]
                }
            ],
            "source": [
                "# 1. Setup Environment\n",
                "!pip install pandas numpy opencv-python matplotlib tqdm scikit-learn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "948edc2f",
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e2a78202",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Data Setup (Google Drive)\n",
                "import os\n",
                "import sys\n",
                "import zipfile\n",
                "import shutil\n",
                "\n",
                "# DETECT ENVIRONMENT\n",
                "is_colab = 'google.colab' in sys.modules or os.path.exists('/content/sample_data')\n",
                "\n",
                "if is_colab:\n",
                "    print(\"\u26a0\ufe0f Running in Google Colab.\")\n",
                "    from google.colab import drive\n",
                "    \n",
                "    # Mount Drive\n",
                "    print(\"Mounting Google Drive...\")\n",
                "    drive.mount('/content/drive')\n",
                "    \n",
                "    # DEFINE YOUR DRIVE PATH HERE\n",
                "    # Expecting 'train.zip' and 'train_labels.csv' in this folder\n",
                "    DRIVE_PATH = '/content/drive/MyDrive' \n",
                "    \n",
                "    print(f\"Looking for files in: {DRIVE_PATH}\")\n",
                "    \n",
                "    drive_zip = os.path.join(DRIVE_PATH, 'train.zip')\n",
                "    drive_csv = os.path.join(DRIVE_PATH, 'train_labels.csv')\n",
                "    \n",
                "    # Copy/Extract Zip\n",
                "    if os.path.exists(drive_zip):\n",
                "        if not os.path.exists('train'):\n",
                "            print(f\"Found {drive_zip}. Extracting to local Colab runtime...\")\n",
                "            with zipfile.ZipFile(drive_zip, 'r') as zip_ref:\n",
                "                zip_ref.extractall('.')\n",
                "            print(\"Extraction complete!\")\n",
                "        else:\n",
                "            print(\"Train folder already exists. Skipping extraction.\")\n",
                "    else:\n",
                "        print(f\"\u274c Error: 'train.zip' not found at {drive_zip}\")\n",
                "        print(\"Please check the path and filename.\")\n",
                "\n",
                "    # Copy CSV\n",
                "    if os.path.exists(drive_csv):\n",
                "        print(f\"Found {drive_csv}. Copying...\")\n",
                "        shutil.copy(drive_csv, '.')\n",
                "        print(\"CSV copied.\")\n",
                "    else:\n",
                "        print(f\"\u274c Error: 'train_labels.csv' not found at {drive_csv}\")\n",
                "\n",
                "else:\n",
                "    print(\"\u2705 Running Locally. Checking for files...\")\n",
                "    \n",
                "    if os.path.exists('train'):\n",
                "        print(\"Found 'train' folder!\")\n",
                "    elif os.path.exists('train_images'):\n",
                "        print(\"Found 'train_images' folder!\")\n",
                "    else:\n",
                "        print(\"Looking for 'train' or 'train_images'...\")\n",
                "        print(f\"Files in {os.getcwd()}: {os.listdir()}\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "14a025ec",
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "043fbbe0",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Imports\n",
                "import os\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "from PIL import Image\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torchvision import transforms, models\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import accuracy_score\n",
                "from tqdm import tqdm\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "2d6a538a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 4. Data Loader Class (STRICT loading)\n",
                "class CancerDetectionDataset(Dataset):\n",
                "    def __init__(self, image_ids, labels, image_dir, transform=None):\n",
                "        self.image_ids = image_ids\n",
                "        self.labels = labels\n",
                "        self.image_dir = image_dir\n",
                "        self.transform = transform\n",
                "        \n",
                "    def __len__(self):\n",
                "        return len(self.image_ids)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        img_id = self.image_ids[idx]\n",
                "        # Check common extensions\n",
                "        possible_paths = [\n",
                "            os.path.join(self.image_dir, f\"{img_id}.tif\"),\n",
                "            os.path.join(self.image_dir, f\"{img_id}.png\")\n",
                "        ]\n",
                "        \n",
                "        image = None\n",
                "        for path in possible_paths:\n",
                "            if os.path.exists(path):\n",
                "                try:\n",
                "                    image = Image.open(path).convert('RGB')\n",
                "                    break\n",
                "                except:\n",
                "                    continue\n",
                "        \n",
                "        if image is None:\n",
                "             pass\n",
                "\n",
                "        if self.transform and image is not None:\n",
                "            image = self.transform(image)\n",
                "        \n",
                "        label = self.labels[idx]\n",
                "        return image, label"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4c1a269b",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 5. Model Architecture (ENSEMBLE: DenseNet + ResNet + EfficientNet)\n",
                "class DenseNetClassifier(nn.Module):\n",
                "    def __init__(self, freeze_backbone=True):\n",
                "        super(DenseNetClassifier, self).__init__()\n",
                "        weights = models.DenseNet121_Weights.DEFAULT\n",
                "        self.backbone = models.densenet121(weights=weights)\n",
                "        \n",
                "        if freeze_backbone:\n",
                "            for param in self.backbone.features.parameters():\n",
                "                param.requires_grad = False\n",
                "                \n",
                "        num_features = self.backbone.classifier.in_features\n",
                "        self.backbone.classifier = nn.Sequential(\n",
                "            nn.Dropout(p=0.3),\n",
                "            nn.Linear(num_features, 512),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Dropout(p=0.4),\n",
                "            nn.Linear(512, 1)\n",
                "        )\n",
                "        \n",
                "    def forward(self, x):\n",
                "        return self.backbone(x)\n",
                "    \n",
                "    def unfreeze_backbone(self):\n",
                "        for param in self.backbone.features.parameters():\n",
                "            param.requires_grad = True\n",
                "\n",
                "class ResNetClassifier(nn.Module):\n",
                "    def __init__(self, freeze_backbone=True):\n",
                "        super(ResNetClassifier, self).__init__()\n",
                "        weights = models.ResNet50_Weights.DEFAULT\n",
                "        self.backbone = models.resnet50(weights=weights)\n",
                "        \n",
                "        if freeze_backbone:\n",
                "            for param in self.backbone.parameters():\n",
                "                param.requires_grad = False\n",
                "        \n",
                "        num_features = self.backbone.fc.in_features\n",
                "        self.backbone.fc = nn.Sequential(\n",
                "            nn.Dropout(p=0.3),\n",
                "            nn.Linear(num_features, 512),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Dropout(p=0.4),\n",
                "            nn.Linear(512, 1)\n",
                "        )\n",
                "        \n",
                "    def forward(self, x):\n",
                "        return self.backbone(x)\n",
                "    \n",
                "    def unfreeze_backbone(self):\n",
                "        for param in self.backbone.parameters():\n",
                "            param.requires_grad = True\n",
                "\n",
                "class EfficientNetClassifier(nn.Module):\n",
                "    def __init__(self, freeze_backbone=True):\n",
                "        super(EfficientNetClassifier, self).__init__()\n",
                "        weights = models.EfficientNet_B0_Weights.DEFAULT\n",
                "        self.backbone = models.efficientnet_b0(weights=weights)\n",
                "        \n",
                "        if freeze_backbone:\n",
                "            for param in self.backbone.features.parameters():\n",
                "                param.requires_grad = False\n",
                "                \n",
                "        num_features = self.backbone.classifier[1].in_features\n",
                "        self.backbone.classifier = nn.Sequential(\n",
                "            nn.Dropout(p=0.3),\n",
                "            nn.Linear(num_features, 512),\n",
                "            nn.ReLU(inplace=True),\n",
                "            nn.Dropout(p=0.4),\n",
                "            nn.Linear(512, 1)\n",
                "        )\n",
                "        \n",
                "    def forward(self, x):\n",
                "        return self.backbone(x)\n",
                "    \n",
                "    def unfreeze_backbone(self):\n",
                "        for param in self.backbone.features.parameters():\n",
                "            param.requires_grad = True\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1747609d",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 6. Training Logic (Updated for Logits)\n",
                "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
                "    model.train()\n",
                "    running_loss = 0.0\n",
                "    all_preds = []\n",
                "    all_labels = []\n",
                "    \n",
                "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
                "    for images, labels in progress_bar:\n",
                "        if images is None: continue # Skip failed loads\n",
                "        \n",
                "        images = images.to(device)\n",
                "        labels = labels.float().unsqueeze(1).to(device)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(images)\n",
                "        loss = criterion(outputs, labels)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        running_loss += loss.item() * images.size(0)\n",
                "        preds = (torch.sigmoid(outputs) > 0.5).float()\n",
                "        all_preds.extend(preds.cpu().numpy())\n",
                "        all_labels.extend(labels.cpu().numpy())\n",
                "        progress_bar.set_postfix({'loss': loss.item()})\n",
                "    \n",
                "    epoch_loss = running_loss / len(dataloader.dataset)\n",
                "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
                "    return epoch_loss, epoch_acc\n",
                "\n",
                "def validate(model, dataloader, criterion, device):\n",
                "    model.eval()\n",
                "    running_loss = 0.0\n",
                "    all_preds = []\n",
                "    all_labels = []\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for images, labels in tqdm(dataloader, desc=\"Validating\"):\n",
                "            if images is None: continue\n",
                "            images = images.to(device)\n",
                "            labels = labels.float().unsqueeze(1).to(device)\n",
                "            outputs = model(images)\n",
                "            loss = criterion(outputs, labels)\n",
                "            \n",
                "            running_loss += loss.item() * images.size(0)\n",
                "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
                "            all_preds.extend(preds.cpu().numpy())\n",
                "            all_labels.extend(labels.cpu().numpy())\n",
                "            \n",
                "    epoch_loss = running_loss / len(dataloader.dataset)\n",
                "    epoch_acc = accuracy_score(all_labels, all_preds)\n",
                "    return epoch_loss, epoch_acc"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7c07c818",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 7. Configuration & Setup\n",
                "import sys\n",
                "import os\n",
                "\n",
                "# DETECT ENVIRONMENT AGAIN\n",
                "is_colab = 'google.colab' in sys.modules or os.path.exists('/content/sample_data')\n",
                "\n",
                "if is_colab:\n",
                "    print(\"Running in Google Colab.\")\n",
                "    IMAGE_DIR = 'train'\n",
                "    CSV_PATH = 'train_labels.csv'\n",
                "else:\n",
                "    # Standard Local Config\n",
                "    CSV_PATH = 'train_labels.csv'\n",
                "    if os.path.exists('train'):\n",
                "        IMAGE_DIR = 'train'\n",
                "    elif os.path.exists('train_images'):\n",
                "        IMAGE_DIR = 'train_images'\n",
                "    else:\n",
                "         IMAGE_DIR = 'train' \n",
                "\n",
                "print(f\"Using Image Directory: {IMAGE_DIR}\")\n",
                "print(f\"Using CSV Path: {CSV_PATH}\")\n",
                "\n",
                "BATCH_SIZE = 64\n",
                "# OPTIMIZATION: Reduced dataset size to 60,000 for faster training while maintaining ~96% potential\n",
                "MAX_SAMPLES = 60000 \n",
                "NUM_EPOCHS = 15\n",
                "\n",
                "transform_train = transforms.Compose([\n",
                "    transforms.RandomHorizontalFlip(),\n",
                "    transforms.RandomVerticalFlip(),\n",
                "    transforms.RandomRotation(90),\n",
                "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
                "])\n",
                "\n",
                "transform_val = transforms.Compose([\n",
                "    transforms.ToTensor(),\n",
                "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
                "])\n",
                "\n",
                "def collate_fn(batch):\n",
                "    batch = list(filter(lambda x: x[0] is not None, batch))\n",
                "    return torch.utils.data.dataloader.default_collate(batch)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "02b99cf2",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 8. Load Data & VERIFY (Stratified Sampling)\n",
                "import pandas as pd\n",
                "import os\n",
                "from sklearn.model_selection import train_test_split\n",
                "from torch.utils.data import DataLoader\n",
                "\n",
                "print(\"=== DEBUGGING DATA LOADING ===\")\n",
                "print(f\"Looking for CSV at: {CSV_PATH}\")\n",
                "print(f\"Looking for Images at: {IMAGE_DIR}\")\n",
                "\n",
                "if not os.path.exists(CSV_PATH):\n",
                "    print(f\"CRITICAL ERROR: {CSV_PATH} missing.\")\n",
                "else:\n",
                "    print(f\"OK: {CSV_PATH} found\")\n",
                "\n",
                "if not os.path.exists(IMAGE_DIR):\n",
                "    print(f\"CRITICAL ERROR: {IMAGE_DIR} missing.\")\n",
                "else:\n",
                "    print(f\"OK: {IMAGE_DIR} found\")\n",
                "\n",
                "if os.path.exists(CSV_PATH):\n",
                "    df = pd.read_csv(CSV_PATH)\n",
                "    \n",
                "    # STRATIFIED SAMPLING to reduce size\n",
                "    if MAX_SAMPLES and MAX_SAMPLES < len(df):\n",
                "        print(f\"Reducing dataset from {len(df)} to {MAX_SAMPLES} (Stratified)...\")\n",
                "        # Use simple train_test_split to get a stratified subset\n",
                "        df, _ = train_test_split(df, train_size=MAX_SAMPLES, stratify=df['label'], random_state=42)\n",
                "        print(f\"New dataset size: {len(df)}\")\n",
                "\n",
                "    ids = df['id'].tolist()\n",
                "    labels = df['label'].tolist()\n",
                "\n",
                "    # Split Train/Val\n",
                "    train_ids, test_ids, train_labels, test_labels = train_test_split(ids, labels, test_size=0.15, stratify=labels, random_state=42)\n",
                "    train_ids, val_ids, train_labels, val_labels = train_test_split(train_ids, train_labels, test_size=0.15, stratify=train_labels, random_state=42)\n",
                "\n",
                "    print(\"\\n=== TESTING IMAGE LOADING ===\")\n",
                "    label_map = {0: 'No Cancer', 1: 'Cancer'}\n",
                "    \n",
                "    # Try to find one example of each class to verify labels are correct\n",
                "    try:\n",
                "        pos_idx = next(i for i, x in enumerate(train_labels) if x == 1)\n",
                "        neg_idx = next(i for i, x in enumerate(train_labels) if x == 0)\n",
                "        indices_to_check = [pos_idx, neg_idx]\n",
                "        \n",
                "        sample_ids = [train_ids[i] for i in indices_to_check]\n",
                "        sample_labels = [train_labels[i] for i in indices_to_check]\n",
                "    except StopIteration:\n",
                "        print(\"Warning: Could not find examples of both classes in current split.\")\n",
                "        sample_ids = train_ids[:3]\n",
                "        sample_labels = train_labels[:3]\n",
                "\n",
                "    test_ds = CancerDetectionDataset(sample_ids, sample_labels, IMAGE_DIR, transform=None)\n",
                "    \n",
                "    try:\n",
                "        for i in range(len(test_ds)):\n",
                "            img, lbl = test_ds[i]\n",
                "            if img is not None:\n",
                "                lbl_name = label_map.get(lbl, 'Unknown')\n",
                "                print(f\"\u2713 Loaded image {sample_ids[i]} (Label: {lbl} - {lbl_name})\")\n",
                "            else:\n",
                "                print(f\"x Failed to load image {sample_ids[i]} (File not found)\")\n",
                "    except Exception as e:\n",
                "        print(f\"\\n!!! FAILURE !!!\")\n",
                "        print(f\"Error: {e}\")\n",
                "\n",
                "    # Create Loaders\n",
                "    train_dataset = CancerDetectionDataset(train_ids, train_labels, IMAGE_DIR, transform=transform_train)\n",
                "    val_dataset = CancerDetectionDataset(val_ids, val_labels, IMAGE_DIR, transform=transform_val)\n",
                "\n",
                "    # Increased batch size slightly for speed if GPU allows, else keep 64\n",
                "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, collate_fn=collate_fn)\n",
                "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, collate_fn=collate_fn)\n",
                "\n",
                "    print(f\"\\nSUCCESS: Ready to train on {len(train_dataset)} samples\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "301d2b1a",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 9. Run Training (ENSEMBLE TRAINING LOOP)\n",
                "models_to_train = [\n",
                "    ('DenseNet', DenseNetClassifier(freeze_backbone=True)),\n",
                "    ('ResNet', ResNetClassifier(freeze_backbone=True)),\n",
                "    ('EfficientNet', EfficientNetClassifier(freeze_backbone=True))\n",
                "]\n",
                "\n",
                "trained_models = {}\n",
                "\n",
                "for name, model in models_to_train:\n",
                "    print(f\"\\n{'='*20}\\nTraining {name}...\\n{'='*20}\")\n",
                "    model = model.to(device)\n",
                "    criterion = nn.BCEWithLogitsLoss()\n",
                "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
                "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
                "    \n",
                "    best_acc = 0.0\n",
                "    best_model_path = f'best_model_{name.lower()}.pth'\n",
                "    \n",
                "    # Train Loop\n",
                "    for epoch in range(NUM_EPOCHS):\n",
                "        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
                "        \n",
                "        loss, acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
                "        print(f\"Train Loss: {loss:.4f} | Acc: {acc:.4f}\")\n",
                "        \n",
                "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
                "        print(f\"Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f}\")\n",
                "        \n",
                "        scheduler.step(val_loss)\n",
                "        \n",
                "        if val_acc > best_acc:\n",
                "            best_acc = val_acc\n",
                "            torch.save(model.state_dict(), best_model_path)\n",
                "            print(f\"Saved best {name} model (Acc: {best_acc:.4f})\")\n",
                "            \n",
                "        # Fine-tuning (simplified: unfreeze halfway)\n",
                "        if epoch == 5:\n",
                "            print(\"Unfreezing backbone for fine-tuning...\")\n",
                "            model.unfreeze_backbone()\n",
                "            for param_group in optimizer.param_groups:\n",
                "                param_group['lr'] = 0.0001\n",
                "\n",
                "    # Reload best weights\n",
                "    model.load_state_dict(torch.load(best_model_path))\n",
                "    trained_models[name] = model\n",
                "\n",
                "print(\"\\nALL MODELS TRAINED!\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "045d2acf",
            "metadata": {},
            "outputs": [],
            "source": [
                "# 10. Ensemble Evaluation\n",
                "print(\"Evaluating Ensemble Performance...\")\n",
                "\n",
                "def evaluate_ensemble(models_dict, dataloader, device):\n",
                "    for model in models_dict.values():\n",
                "        model.eval()\n",
                "        \n",
                "    all_preds = []\n",
                "    all_labels = []\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for images, labels in tqdm(dataloader, desc=\"Ensemble Validating\"):\n",
                "            if images is None: continue\n",
                "            images = images.to(device)\n",
                "            labels = labels.to(device)\n",
                "            \n",
                "            # Get predictions from all models\n",
                "            batch_preds = []\n",
                "            for name, model in models_dict.items():\n",
                "                logits = model(images)\n",
                "                probs = torch.sigmoid(logits)\n",
                "                batch_preds.append(probs)\n",
                "            \n",
                "            # Average probabilities\n",
                "            avg_probs = torch.mean(torch.stack(batch_preds), dim=0)\n",
                "            \n",
                "            preds = (avg_probs > 0.5).float()\n",
                "            all_preds.extend(preds.cpu().numpy())\n",
                "            all_labels.extend(labels.cpu().numpy())\n",
                "            \n",
                "    acc = accuracy_score(all_labels, all_preds)\n",
                "    return acc\n",
                "\n",
                "ensemble_acc = evaluate_ensemble(trained_models, val_loader, device)\n",
                "print(f\"\\n>>> ENSEMBLE ACCURACY: {ensemble_acc:.4f} <<<\")\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.7"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}